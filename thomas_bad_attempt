{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_attempt_no_relu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Z-O16QDJqpV",
        "outputId": "2bedc91e-87f2-455e-c509-bb07b351df9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJiVsQSFJqpk",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, filepath):\n",
        "        cwd = os.getcwd()\n",
        "        self.basepath = filepath\n",
        "        try:\n",
        "            os.stat(self.basepath+\"/add_prim_split\")\n",
        "            os.stat(self.basepath+\"/few_shot_split\")\n",
        "            os.stat(self.basepath+\"/filler_split\")\n",
        "            os.stat(self.basepath+\"/length_split\")\n",
        "            os.stat(self.basepath+\"/simple_split\")\n",
        "            os.stat(self.basepath+\"/template_split\")\n",
        "        except Exception as e:\n",
        "            raise Exception(\"Path \"+filepath+\" doesnt seem to contain the required folders.\")\n",
        "\n",
        "    def load_1a(self):\n",
        "        train = self.file_loader(\"/simple_split/tasks_train_simple.txt\")\n",
        "        test = self.file_loader(\"/simple_split/tasks_test_simple.txt\")\n",
        "\n",
        "        return (np.asarray(train), np.asarray(test))\n",
        "\n",
        "    def load_1b(self):\n",
        "        percentile_dict = {}\n",
        "        splits = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"]\n",
        "\n",
        "        for percentile in splits:\n",
        "            train = self.file_loader(\"/simple_split/size_variations/tasks_train_simple_p{}.txt\".format(percentile))\n",
        "            test = self.file_loader(\"/simple_split/size_variations/tasks_test_simple_p{}.txt\".format(percentile))\n",
        "            \n",
        "            percentile_dict[percentile] = (np.asarray(train), np.asarray(test))\n",
        "            \n",
        "        return percentile_dict\n",
        "\n",
        "    def load_2(self):\n",
        "        train = self.file_loader(\"/length_split/tasks_train_length.txt\")\n",
        "        test = self.file_loader(\"/length_split/tasks_test_length.txt\")\n",
        "\n",
        "        return (np.asarray(train), np.asarray(test))\n",
        "\n",
        "    def load_3(self):\n",
        "        \"\"\"\n",
        "        loads the datasets for both parts of the experiment\n",
        "        the first part where both primitives appear without compositional commands\n",
        "        the second part where 'jump' primitive appears in\n",
        "        compositional commands of varying lengths\n",
        "        returns a dictionary of pairs all possible train/test sets\n",
        "        \"\"\"\n",
        "        data_dict = {}\n",
        "        nums = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n",
        "        reps = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "\n",
        "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_jump.txt\")\n",
        "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_jump.txt\")\n",
        "        data_dict['jump'] = (np.asarray(train), np.asarray(test))\n",
        "\n",
        "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_turn_left.txt\")\n",
        "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_turn_left.txt\")\n",
        "        data_dict['lturn'] = (np.asarray(train), np.asarray(test))\n",
        "        \n",
        "        for num in nums:\n",
        "            for rep in reps:\n",
        "                train = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
        "                test = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
        "                \n",
        "                data_dict['jump_num{}_rep{}'.format(num, rep)] = (np.asarray(train), np.asarray(test))\n",
        "            \n",
        "        return data_dict\n",
        "\n",
        "    def file_loader(self, path):\n",
        "        sent_list = []\n",
        "        with open(self.basepath+path, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        sent_list.append(line_splitter(line))\n",
        "        return sent_list\n",
        "\n",
        "    \n",
        "def line_splitter(sentence):\n",
        "    sent_list = sentence.split(\"OUT: \")\n",
        "    sent_list[0] = sent_list[0].strip(\"IN: \")\n",
        "    sent_list[1] = sent_list[1].strip(\"\\n\")\n",
        "\n",
        "    return sent_list\n",
        "\n",
        "# examples:\n",
        "# 1a :\n",
        "#   train, test = dl.load_1a()\n",
        "#   train[0][0] first train sentence, \"IN\"\n",
        "#   train[0][1] first train sentence, \"OUT\"\n",
        "# 1b :\n",
        "#   dict = dl.load_1b()\n",
        "#   train, test = dict[\"1\"] extract the 1 percentile sentences out, split into train and test\n",
        "#   train[0][0] first train sentence, \"OUT\"\n",
        "#   train[0][1] first train sentence, \"OUT\"\n",
        "#\n",
        "# all returns are numpy arrays\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "whB7XHBzJqps",
        "colab": {}
      },
      "source": [
        "#from data_loader import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Input:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {EOS_token: \"EOS\"}\n",
        "        #self.index2word = {}\n",
        "        self.n_words = 1  # Count SOS and EOS\n",
        "        #self.n_words = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "class Output:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        #self.index2word = {}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "        #self.n_words = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "        \n",
        "def get_embedding(word, lookup_dict, embeds):\n",
        "    tensor = torch.tensor([lookup_dict[word]], dtype=torch.long)\n",
        "    return embeds(tensor)\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, output_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FZ1BIiplJqpy",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CustomLoss(torch.autograd.Function):  \n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        import ipdb; ipdb.set_trace()\n",
        "        #pass\n",
        "        return\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.0, layers=1, mode='RNN'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.layers = layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        self.hidden_layer = nn.RNN(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "\n",
        "        if mode == 'LSTM':\n",
        "        \tself.hidden_layer = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "        elif mode == 'GRU':\n",
        "        \tself.hidden_layer = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output, hidden = self.hidden_layer(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        hidden = torch.zeros(self.layers, 1, self.hidden_size, device=device)\n",
        "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.0, layers=1, attention=False, mode='RNN'):\n",
        "    \t#layers should be either 1 or 2\n",
        "    \t#in the latter case remember to pass a pair of hidden states!\n",
        "    \t#mode can be either 'LSTM', 'GRU' or 'RNN'\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.layers = layers\n",
        "        self.max_length = max_length\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        if self.attention:\n",
        "\t        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "\t        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        self.hidden_layer = nn.RNN(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "\n",
        "        if mode == 'LSTM':\n",
        "        \tself.hidden_layer = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "        elif mode == 'GRU':\n",
        "        \tself.hidden_layer = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs=None):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        if self.attention:\n",
        "\t        attn_weights = F.softmax(\n",
        "\t            self.attn(torch.cat((output[0], hidden[0][0]), 1)), dim=1)\n",
        "\t        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "\t        output = torch.cat((output[0], attn_applied[0]), 1)\n",
        "        \toutput = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.hidden_layer(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        hidden = torch.zeros(self.layers, 1, self.hidden_size, device=device)\n",
        "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
        "        return hidden\n",
        "\n",
        "def train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length, clipping_value=5, mode='RNN'):\n",
        "    encoder_hidden1 = encoder.initHidden()\n",
        "    encoder_hidden2 = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    output_length = output_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        if mode == 'LSTM':\n",
        "            encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei], (encoder_hidden1, encoder_hidden2))\n",
        "        else: \n",
        "            encoder_output, encoder_hidden1 = encoder(input_tensor[ei], encoder_hidden1)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "    \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden1 = encoder_hidden1\n",
        "    decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "    forcing = random.random() > 0.5\n",
        "\n",
        "    if forcing:\n",
        "        for di in range(output_length):\n",
        "            if mode == 'LSTM':\n",
        "                decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "            decoder_input = output_tensor[di]\n",
        "            loss += criterion(decoder_output, output_tensor[di])\n",
        "\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    else:\n",
        "        for di in range(output_length):\n",
        "            if mode == 'LSTM':\n",
        "                decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, output_tensor[di])\n",
        "            \n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    #loss = CustomLoss\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clipping_value)\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clipping_value)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item() / output_length\n",
        "\n",
        "    \n",
        "def trainIters(encoder, decoder, train_data, input_lang, output_lang, max_length, learning_rate=0.001, mode='RNN'):\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    losses = []\n",
        "    print(train_data.shape[0])\n",
        "    print_loss_total = 0\n",
        "\n",
        "    for iter in range(train_data.shape[0]):\n",
        "        training_pair = tensorsFromPair(train_data[iter], input_lang, output_lang)\n",
        "        input_tensor = training_pair[0]\n",
        "        output_tensor = training_pair[1]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input_tensor = input_tensor.cuda()\n",
        "            output_tensor = output_tensor.cuda()\n",
        "        \n",
        "        loss = train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length, mode=mode)\n",
        "        losses.append(loss)\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            print_loss_avg = print_loss_total / 500\n",
        "            print(iter)\n",
        "            print(print_loss_avg)\n",
        "            print_loss_total = 0\n",
        "\n",
        "    return losses\n",
        "\n",
        "dl = DataLoader(\"/content/drive/My Drive/Colab Notebooks/SCAN\")\n",
        "#dl = DataLoader(\"SCAN\")\n",
        "train_data, test_data = dl.load_1a()\n",
        "\n",
        "MAX_LENGTH = max([len(x[0].split()) for x in train_data]) + 1\n",
        "\n",
        "train_in = Input(\"train_input\")\n",
        "train_out = Output(\"train_output\")\n",
        "\n",
        "test_in = Input(\"test_input\")\n",
        "test_out = Output(\"test_output\")\n",
        "\n",
        "for datapoint in train_data:\n",
        "        train_in.addSentence(datapoint[0])\n",
        "        train_out.addSentence(datapoint[1])\n",
        "\n",
        "for datapoint in test_data:\n",
        "        test_in.addSentence(datapoint[0])\n",
        "        test_out.addSentence(datapoint[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66NZZ_dOKUg9",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "train_data = train_data[np.random.choice(train_data.shape[0], 100000, replace=True), :]\n",
        "\n",
        "def train_and_save(model, dropout, att, layers, model_name):\n",
        "    encoder = Encoder(train_in.n_words, 200, layers=layers, mode=model, dropout_p=dropout)\n",
        "    decoder = Decoder(200, train_out.n_words, layers=layers, max_length=MAX_LENGTH, mode=model, dropout_p=dropout, attention=att)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "    losses = trainIters(encoder, decoder, train_data, train_in, train_out, MAX_LENGTH, mode=model)\n",
        "    plt.plot(losses)\n",
        "    plt.title(model+'_layers='+str(layers)+'_drop='+str(dropout)+'_attention='+str(att))\n",
        "    plt.xlabel('iterations')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()\n",
        "    torch.save(encoder.state_dict(), \"/content/drive/My Drive/\"+model_name+\"_encoder.pt\")\n",
        "    torch.save(decoder.state_dict(), \"/content/drive/My Drive/\"+model_name+\"_decoder.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NwGTOeF7Jqp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51ebd5e0-2699-4abc-ed37-5bbbb06b3a4c"
      },
      "source": [
        "model='LSTM'\n",
        "train_and_save(model, dropout=0.0, att=False, layers=2, model_name='LSTM_no_drop_no_att_2_layer_no_relu.pt')\n",
        "#train_and_save(model, 0.0, False, layers=2, 'srn_att_drop')\n",
        "#train_and_save(model, 0.0, True, 2, 'srn_att_drop')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "0\n",
            "0.004151281356811524\n",
            "500\n",
            "1.5077190357907184\n",
            "1000\n",
            "1.1485740495830519\n",
            "1500\n",
            "0.9003050487158801\n",
            "2000\n",
            "0.7478588577232645\n",
            "2500\n",
            "0.5864907503519277\n",
            "3000\n",
            "0.5182694460750862\n",
            "3500\n",
            "0.46473863003314575\n",
            "4000\n",
            "0.40226149665937794\n",
            "4500\n",
            "0.35824245508816055\n",
            "5000\n",
            "0.33816720894520425\n",
            "5500\n",
            "0.31580045190332773\n",
            "6000\n",
            "0.27205810395059526\n",
            "6500\n",
            "0.26117205923828596\n",
            "7000\n",
            "0.22657106388478973\n",
            "7500\n",
            "0.23916588162545394\n",
            "8000\n",
            "0.159394235544148\n",
            "8500\n",
            "0.19504597094966095\n",
            "9000\n",
            "0.1980500007361625\n",
            "9500\n",
            "0.2050438690441494\n",
            "10000\n",
            "0.1331849195352721\n",
            "10500\n",
            "0.12847256207648747\n",
            "11000\n",
            "0.15138879632320493\n",
            "11500\n",
            "0.1118898346315442\n",
            "12000\n",
            "0.10755830268100582\n",
            "12500\n",
            "0.09462439921662255\n",
            "13000\n",
            "0.10445188599692737\n",
            "13500\n",
            "0.07990304651168002\n",
            "14000\n",
            "0.1103378758966057\n",
            "14500\n",
            "0.07586367855078342\n",
            "15000\n",
            "0.07176685820311683\n",
            "15500\n",
            "0.11189796127902771\n",
            "16000\n",
            "0.0735191657884956\n",
            "16500\n",
            "0.09178749345050158\n",
            "17000\n",
            "0.07290721572344536\n",
            "17500\n",
            "0.07179129954223029\n",
            "18000\n",
            "0.087665699688035\n",
            "18500\n",
            "0.05077968955638298\n",
            "19000\n",
            "0.06576965296750058\n",
            "19500\n",
            "0.0456377538743369\n",
            "20000\n",
            "0.05860976995789218\n",
            "20500\n",
            "0.044182119043741246\n",
            "21000\n",
            "0.037265670329560616\n",
            "21500\n",
            "0.051551681085377415\n",
            "22000\n",
            "0.0698800126898364\n",
            "22500\n",
            "0.05219539919039632\n",
            "23000\n",
            "0.04415532476355738\n",
            "23500\n",
            "0.057623068304871315\n",
            "24000\n",
            "0.049879014678865685\n",
            "24500\n",
            "0.028519394310973962\n",
            "25000\n",
            "0.05527320693300209\n",
            "25500\n",
            "0.07163390849385369\n",
            "26000\n",
            "0.04766298790870081\n",
            "26500\n",
            "0.04411405688726274\n",
            "27000\n",
            "0.060568604690394545\n",
            "27500\n",
            "0.048793678175682205\n",
            "28000\n",
            "0.03752636796899834\n",
            "28500\n",
            "0.054362742411937275\n",
            "29000\n",
            "0.05948437406841054\n",
            "29500\n",
            "0.06009959723994842\n",
            "30000\n",
            "0.026401559026756678\n",
            "30500\n",
            "0.029885710759939347\n",
            "31000\n",
            "0.06383800809095219\n",
            "31500\n",
            "0.048268865199697426\n",
            "32000\n",
            "0.03683579351979661\n",
            "32500\n",
            "0.03328745427120998\n",
            "33000\n",
            "0.03638512438520749\n",
            "33500\n",
            "0.07050634407152968\n",
            "34000\n",
            "0.018192664946534657\n",
            "34500\n",
            "0.02583288485340391\n",
            "35000\n",
            "0.036414348783375544\n",
            "35500\n",
            "0.05730667691312406\n",
            "36000\n",
            "0.037000358606334674\n",
            "36500\n",
            "0.059351860583334554\n",
            "37000\n",
            "0.052437676409124154\n",
            "37500\n",
            "0.053400392863833274\n",
            "38000\n",
            "0.06581790368235585\n",
            "38500\n",
            "0.028919558633325914\n",
            "39000\n",
            "0.048857824220248196\n",
            "39500\n",
            "0.0218217535187757\n",
            "40000\n",
            "0.036411146869371275\n",
            "40500\n",
            "0.032866189641994545\n",
            "41000\n",
            "0.03399433097584994\n",
            "41500\n",
            "0.034494274587277586\n",
            "42000\n",
            "0.041345747621574135\n",
            "42500\n",
            "0.03472207388473906\n",
            "43000\n",
            "0.03505057913494582\n",
            "43500\n",
            "0.08460852206225411\n",
            "44000\n",
            "0.04322642022124719\n",
            "44500\n",
            "0.033633971889535576\n",
            "45000\n",
            "0.0229567920975267\n",
            "45500\n",
            "0.041282898076941854\n",
            "46000\n",
            "0.028919180311784417\n",
            "46500\n",
            "0.03435634293959498\n",
            "47000\n",
            "0.03285714285162965\n",
            "47500\n",
            "0.033126163865369936\n",
            "48000\n",
            "0.029088279059689897\n",
            "48500\n",
            "0.02037216152736568\n",
            "49000\n",
            "0.03702478797775219\n",
            "49500\n",
            "0.05430766806569262\n",
            "50000\n",
            "0.050000135531736364\n",
            "50500\n",
            "0.03395354033563582\n",
            "51000\n",
            "0.027210100934238306\n",
            "51500\n",
            "0.024429649366335984\n",
            "52000\n",
            "0.03249900152208282\n",
            "52500\n",
            "0.029044866395562165\n",
            "53000\n",
            "0.07517061397398885\n",
            "53500\n",
            "0.024728328622408535\n",
            "54000\n",
            "0.03408240692445375\n",
            "54500\n",
            "0.02149417031432212\n",
            "55000\n",
            "0.02684183756042879\n",
            "55500\n",
            "0.032029768809912686\n",
            "56000\n",
            "0.01955880268660397\n",
            "56500\n",
            "0.03366459144864423\n",
            "57000\n",
            "0.03447312575836345\n",
            "57500\n",
            "0.0411230122148977\n",
            "58000\n",
            "0.041488037290959255\n",
            "58500\n",
            "0.03024724880866075\n",
            "59000\n",
            "0.046429412649024185\n",
            "59500\n",
            "0.04855633842307574\n",
            "60000\n",
            "0.031007925313934117\n",
            "60500\n",
            "0.028690453723412345\n",
            "61000\n",
            "0.026155778356110947\n",
            "61500\n",
            "0.029883654372843425\n",
            "62000\n",
            "0.02950172735749511\n",
            "62500\n",
            "0.034825991602737624\n",
            "63000\n",
            "0.01788229720439456\n",
            "63500\n",
            "0.039105841105540426\n",
            "64000\n",
            "0.03082870738049922\n",
            "64500\n",
            "0.020668031697829173\n",
            "65000\n",
            "0.032000266909119016\n",
            "65500\n",
            "0.01930667537062429\n",
            "66000\n",
            "0.03783360499051818\n",
            "66500\n",
            "0.10895620930907443\n",
            "67000\n",
            "0.04338851146759892\n",
            "67500\n",
            "0.06590830359398686\n",
            "68000\n",
            "0.030417047645754505\n",
            "68500\n",
            "0.032751477531421366\n",
            "69000\n",
            "0.013087327871378509\n",
            "69500\n",
            "0.048802832758743385\n",
            "70000\n",
            "0.04856081968960969\n",
            "70500\n",
            "0.06413206663572857\n",
            "71000\n",
            "0.03579034898969407\n",
            "71500\n",
            "0.013364315823045272\n",
            "72000\n",
            "0.01070464565288887\n",
            "72500\n",
            "0.04196525300346958\n",
            "73000\n",
            "0.05063959456804342\n",
            "73500\n",
            "0.02724956501850191\n",
            "74000\n",
            "0.012306323167403623\n",
            "74500\n",
            "0.036832789349510016\n",
            "75000\n",
            "0.016322181494578543\n",
            "75500\n",
            "0.04712154292525084\n",
            "76000\n",
            "0.028632462766091927\n",
            "76500\n",
            "0.015661639975184972\n",
            "77000\n",
            "0.038378545105684427\n",
            "77500\n",
            "0.03193819550448199\n",
            "78000\n",
            "0.015948139320162858\n",
            "78500\n",
            "0.031063931444426165\n",
            "79000\n",
            "0.03075629963426191\n",
            "79500\n",
            "0.03988112613467532\n",
            "80000\n",
            "0.05578251110297456\n",
            "80500\n",
            "0.02857322235340845\n",
            "81000\n",
            "0.012237060437951726\n",
            "81500\n",
            "0.038905832730640424\n",
            "82000\n",
            "0.02311441490742315\n",
            "82500\n",
            "0.04808272071880214\n",
            "83000\n",
            "0.023433106078820033\n",
            "83500\n",
            "0.05420067595405427\n",
            "84000\n",
            "0.03213615491947248\n",
            "84500\n",
            "0.021298125348217158\n",
            "85000\n",
            "0.0332585541522467\n",
            "85500\n",
            "0.022584214539273628\n",
            "86000\n",
            "0.05737289280249076\n",
            "86500\n",
            "0.03702658134546753\n",
            "87000\n",
            "0.04679457036692783\n",
            "87500\n",
            "0.06785737188752158\n",
            "88000\n",
            "0.07397804579098605\n",
            "88500\n",
            "0.0435157596124562\n",
            "89000\n",
            "0.09741773948402066\n",
            "89500\n",
            "0.03955081711708235\n",
            "90000\n",
            "0.03232217000851378\n",
            "90500\n",
            "0.03034429320546979\n",
            "91000\n",
            "0.03812417276219794\n",
            "91500\n",
            "0.03128445578018774\n",
            "92000\n",
            "0.0625262347592212\n",
            "92500\n",
            "0.04791791575606788\n",
            "93000\n",
            "0.06483454031051131\n",
            "93500\n",
            "0.028163508224746163\n",
            "94000\n",
            "0.07656590043591784\n",
            "94500\n",
            "0.05336810906895133\n",
            "95000\n",
            "0.025520022607608134\n",
            "95500\n",
            "0.04224148180753387\n",
            "96000\n",
            "0.041802105313220525\n",
            "96500\n",
            "0.04787087503965454\n",
            "97000\n",
            "0.06727153794052565\n",
            "97500\n",
            "0.06769973385233953\n",
            "98000\n",
            "0.045532587243705167\n",
            "98500\n",
            "0.05370107189091094\n",
            "99000\n",
            "0.028540340862453047\n",
            "99500\n",
            "0.04934679469398285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAGDCAYAAAD6aR7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc_TWONzzvlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "50ed268a-ca8c-439b-a6c7-e9334a323ba6"
      },
      "source": [
        "encoder = Encoder(train_in.n_words, 200, layers=2, mode=\"LSTM\", dropout_p=0.0)\n",
        "encoder.load_state_dict(torch.load(\"/content/drive/My Drive/lstm_no_att_no_drop_encoder.pt\"))\n",
        "encoder.eval()\n",
        "\n",
        "decoder = Decoder(200, train_out.n_words, layers=2, max_length=MAX_LENGTH, mode=\"LSTM\", dropout_p=0.0, attention=False)\n",
        "decoder.load_state_dict(torch.load(\"/content/drive/My Drive/lstm_no_att_no_drop_decoder.pt\"))\n",
        "decoder.eval()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(8, 200)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (hidden_layer): LSTM(200, 200, num_layers=2)\n",
              "  (out): Linear(in_features=200, out_features=8, bias=True)\n",
              "  (softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pxmS4rEHL366",
        "outputId": "39719bf9-8ee3-4ffc-de17-bee35fedad60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "#from layers_attempt import *\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device=torch.device(\"cpu\")\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length=100):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        #if torch.cuda.is_available():\n",
        "        #    input_tensor = input_tensor.cuda()\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden1 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "        encoder_hidden2 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei],\n",
        "                                                     (encoder_hidden1, encoder_hidden2))\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "        decoder_hidden1 = encoder_hidden1\n",
        "        decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "          if decoder.attention:\n",
        "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs[:10]) # needs to only have the 10 in the encoder outputs?\n",
        "          else:\n",
        "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "          topv, topi = decoder_output.data.topk(1)\n",
        "          if topi.item() == EOS_token:\n",
        "              #decoded_words.append('<EOS>')\n",
        "              break\n",
        "          else:\n",
        "            decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "        return decoded_words\n",
        "\n",
        "def evaluateIters(test_data, encoder, decoder, lang_in, lang_out):\n",
        "    hit = 0\n",
        "    miss = 0\n",
        "    iters = 0\n",
        "    hit_idx = []\n",
        "    miss_idx = []\n",
        "\n",
        "    for idx, test_point in enumerate(test_data):\n",
        "        pred = evaluate(encoder, decoder, test_point[0], lang_in, lang_out)\n",
        "        pred = \" \".join(pred)\n",
        "        if pred == test_point[1]:\n",
        "            hit += 1\n",
        "            hit_idx.append(idx)\n",
        "        else:\n",
        "            miss += 1\n",
        "            miss_idx.append(idx)\n",
        "        iters += 1\n",
        "\n",
        "        if iters % 100 == 0:\n",
        "            print(iters)\n",
        "            print(hit)\n",
        "\n",
        "    return hit, hit_idx, miss, miss_idx\n",
        "\n",
        "hit, hit_idx, miss, miss_idx = evaluateIters(test_data, encoder, decoder, train_in, train_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "3\n",
            "200\n",
            "7\n",
            "300\n",
            "10\n",
            "400\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "10WjnZrGR8aD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf4b8de-9d00-406d-9c45-b39d43f684b5"
      },
      "source": [
        "1-miss/test_data.shape[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5098039215686274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9auy0X5hXmD8",
        "outputId": "e95c910b-4b5c-42a0-f812-68c2522c1469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "idx=50\n",
        "print(test_data[miss_idx[idx]][0].split())\n",
        "print(evaluate(encoder, decoder, test_data[miss_idx[idx]][0], train_in, train_out))\n",
        "print(len(evaluate(encoder, decoder, test_data[miss_idx[idx]][0], train_in, train_out)))\n",
        "print(test_data[miss_idx[idx]][1].split())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['run', 'twice', 'after', 'jump', 'around', 'left']\n",
            "['I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_RUN', 'I_RUN', 'I_RUN']\n",
            "10\n",
            "['I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_RUN', 'I_RUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EG638DGIa-je",
        "outputId": "2ef8e446-d0c6-4e9e-df70-9386e416b213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data[4000][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'turn left thrice after turn opposite right twice'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFn0WQ1kU0VG",
        "outputId": "bd34bec0-3b01-4241-9f7a-1ceb74ab466c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max([len(x[0].split()) for x in train_data])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jJcgD6Z4VaFS",
        "outputId": "6e46323f-ccbf-4240-83b7-83c8762c3ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "str(True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'True'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSWSsqUHS3Rc",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}