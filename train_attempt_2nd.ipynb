{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_attempt_no_relu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Z-O16QDJqpV",
        "outputId": "83890055-8f05-41ac-a008-5e982b5f2489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJiVsQSFJqpk",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, filepath):\n",
        "        cwd = os.getcwd()\n",
        "        self.basepath = filepath\n",
        "        try:\n",
        "            os.stat(self.basepath+\"/add_prim_split\")\n",
        "            os.stat(self.basepath+\"/few_shot_split\")\n",
        "            os.stat(self.basepath+\"/filler_split\")\n",
        "            os.stat(self.basepath+\"/length_split\")\n",
        "            os.stat(self.basepath+\"/simple_split\")\n",
        "            os.stat(self.basepath+\"/template_split\")\n",
        "        except Exception as e:\n",
        "            raise Exception(\"Path \"+filepath+\" doesnt seem to contain the required folders.\")\n",
        "\n",
        "    def load_1a(self):\n",
        "        train = self.file_loader(\"/simple_split/tasks_train_simple.txt\")\n",
        "        test = self.file_loader(\"/simple_split/tasks_test_simple.txt\")\n",
        "\n",
        "        return (np.asarray(train), np.asarray(test))\n",
        "\n",
        "    def load_1b(self):\n",
        "        percentile_dict = {}\n",
        "        splits = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"]\n",
        "\n",
        "        for percentile in splits:\n",
        "            train = self.file_loader(\"/simple_split/size_variations/tasks_train_simple_p{}.txt\".format(percentile))\n",
        "            test = self.file_loader(\"/simple_split/size_variations/tasks_test_simple_p{}.txt\".format(percentile))\n",
        "            \n",
        "            percentile_dict[percentile] = (np.asarray(train), np.asarray(test))\n",
        "            \n",
        "        return percentile_dict\n",
        "\n",
        "    def load_2(self):\n",
        "        train = self.file_loader(\"/length_split/tasks_train_length.txt\")\n",
        "        test = self.file_loader(\"/length_split/tasks_test_length.txt\")\n",
        "\n",
        "        return (np.asarray(train), np.asarray(test))\n",
        "\n",
        "    def load_3(self):\n",
        "        \"\"\"\n",
        "        loads the datasets for both parts of the experiment\n",
        "        the first part where both primitives appear without compositional commands\n",
        "        the second part where 'jump' primitive appears in\n",
        "        compositional commands of varying lengths\n",
        "        returns a dictionary of pairs all possible train/test sets\n",
        "        \"\"\"\n",
        "        data_dict = {}\n",
        "        nums = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n",
        "        reps = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "\n",
        "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_jump.txt\")\n",
        "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_jump.txt\")\n",
        "        data_dict['jump'] = (np.asarray(train), np.asarray(test))\n",
        "\n",
        "        train = self.file_loader(\"/add_prim_split/tasks_train_addprim_turn_left.txt\")\n",
        "        test = self.file_loader(\"/add_prim_split/tasks_test_addprim_turn_left.txt\")\n",
        "        data_dict['lturn'] = (np.asarray(train), np.asarray(test))\n",
        "        \n",
        "        for num in nums:\n",
        "            for rep in reps:\n",
        "                train = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
        "                test = self.file_loader(\"/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num{}_rep{}.txt\".format(num, rep))\n",
        "                \n",
        "                data_dict['jump_num{}_rep{}'.format(num, rep)] = (np.asarray(train), np.asarray(test))\n",
        "            \n",
        "        return data_dict\n",
        "\n",
        "    def file_loader(self, path):\n",
        "        sent_list = []\n",
        "        with open(self.basepath+path, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        sent_list.append(line_splitter(line))\n",
        "        return sent_list\n",
        "\n",
        "    \n",
        "def line_splitter(sentence):\n",
        "    sent_list = sentence.split(\"OUT: \")\n",
        "    sent_list[0] = sent_list[0].strip(\"IN: \")\n",
        "    sent_list[1] = sent_list[1].strip(\"\\n\")\n",
        "\n",
        "    return sent_list\n",
        "\n",
        "# examples:\n",
        "# 1a :\n",
        "#   train, test = dl.load_1a()\n",
        "#   train[0][0] first train sentence, \"IN\"\n",
        "#   train[0][1] first train sentence, \"OUT\"\n",
        "# 1b :\n",
        "#   dict = dl.load_1b()\n",
        "#   train, test = dict[\"1\"] extract the 1 percentile sentences out, split into train and test\n",
        "#   train[0][0] first train sentence, \"OUT\"\n",
        "#   train[0][1] first train sentence, \"OUT\"\n",
        "#\n",
        "# all returns are numpy arrays\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "whB7XHBzJqps",
        "colab": {}
      },
      "source": [
        "#from data_loader import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Input:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {EOS_token: \"EOS\"}\n",
        "        #self.index2word = {}\n",
        "        self.n_words = 1  # Count SOS and EOS\n",
        "        #self.n_words = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "class Output:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        #self.index2word = {}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "        #self.n_words = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "        \n",
        "def get_embedding(word, lookup_dict, embeds):\n",
        "    tensor = torch.tensor([lookup_dict[word]], dtype=torch.long)\n",
        "    return embeds(tensor)\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, output_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FZ1BIiplJqpy",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CustomLoss(torch.autograd.Function):  \n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        import ipdb; ipdb.set_trace()\n",
        "        #pass\n",
        "        return\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.0, layers=1, mode='RNN'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.layers = layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        self.hidden_layer = nn.RNN(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "\n",
        "        if mode == 'LSTM':\n",
        "        \tself.hidden_layer = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "        elif mode == 'GRU':\n",
        "        \tself.hidden_layer = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output, hidden = self.hidden_layer(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        hidden = torch.zeros(self.layers, 1, self.hidden_size, device=device)\n",
        "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.0, layers=1, attention=False, mode='RNN'):\n",
        "    \t#layers should be either 1 or 2\n",
        "    \t#in the latter case remember to pass a pair of hidden states!\n",
        "    \t#mode can be either 'LSTM', 'GRU' or 'RNN'\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.layers = layers\n",
        "        self.max_length = max_length\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        if self.attention:\n",
        "\t        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "\t        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        self.hidden_layer = nn.RNN(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "\n",
        "        if mode == 'LSTM':\n",
        "        \tself.hidden_layer = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "        elif mode == 'GRU':\n",
        "        \tself.hidden_layer = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.layers)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs=None):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        if self.attention:\n",
        "\t        attn_weights = F.softmax(\n",
        "\t            self.attn(torch.cat((output[0], hidden[0][0]), 1)), dim=1)\n",
        "\t        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "\t        output = torch.cat((output[0], attn_applied[0]), 1)\n",
        "        \toutput = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.hidden_layer(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        hidden = torch.zeros(self.layers, 1, self.hidden_size, device=device)\n",
        "        nn.init.xavier_uniform_(hidden, gain=nn.init.calculate_gain('relu'))\n",
        "        return hidden\n",
        "\n",
        "def train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length, clipping_value=5, mode='RNN'):\n",
        "    encoder_hidden1 = encoder.initHidden()\n",
        "    encoder_hidden2 = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    output_length = output_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        if mode == 'LSTM':\n",
        "            encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei], (encoder_hidden1, encoder_hidden2))\n",
        "        else: \n",
        "            encoder_output, encoder_hidden1 = encoder(input_tensor[ei], encoder_hidden1)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "    \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden1 = encoder_hidden1\n",
        "    decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "    forcing = random.random() > 0.5\n",
        "\n",
        "    if forcing:\n",
        "        for di in range(output_length):\n",
        "            if mode == 'LSTM':\n",
        "                decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "            decoder_input = output_tensor[di]\n",
        "            loss += criterion(decoder_output, output_tensor[di])\n",
        "\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    else:\n",
        "        for di in range(output_length):\n",
        "            if mode == 'LSTM':\n",
        "                decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden1 = decoder(decoder_input, decoder_hidden1, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, output_tensor[di])\n",
        "            \n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    #loss = CustomLoss\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clipping_value)\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clipping_value)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item() / output_length\n",
        "\n",
        "    \n",
        "def trainIters(encoder, decoder, train_data, input_lang, output_lang, max_length, learning_rate=0.001, mode='RNN'):\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    losses = []\n",
        "    print(train_data.shape[0])\n",
        "    print_loss_total = 0\n",
        "\n",
        "    for iter in range(train_data.shape[0]):\n",
        "        training_pair = tensorsFromPair(train_data[iter], input_lang, output_lang)\n",
        "        input_tensor = training_pair[0]\n",
        "        output_tensor = training_pair[1]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input_tensor = input_tensor.cuda()\n",
        "            output_tensor = output_tensor.cuda()\n",
        "        \n",
        "        loss = train(input_tensor, output_tensor, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, max_length, mode=mode)\n",
        "        losses.append(loss)\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            print_loss_avg = print_loss_total / 500\n",
        "            print(iter)\n",
        "            print(print_loss_avg)\n",
        "            print_loss_total = 0\n",
        "\n",
        "    return losses\n",
        "\n",
        "dl = DataLoader(\"/content/drive/My Drive/Colab Notebooks/SCAN\")\n",
        "#dl = DataLoader(\"SCAN\")\n",
        "\n",
        "#train_data, test_data = dl.load_1a()\n",
        "data_dict = dl.load_1b()\n",
        "\n",
        "splits = [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"]\n",
        "\n",
        "'''\n",
        "train_data_1, test_data_1 = data_dict[\"1\"]\n",
        "train_data_2, test_data_2 = data_dict[\"2\"]\n",
        "train_data_4, test_data_4 = data_dict[\"4\"]\n",
        "train_data_8, test_data_8 = data_dict[\"8\"]\n",
        "train_data_16, test_data_16 = data_dict[\"16\"]\n",
        "train_data_32 test_data_32 = data_dict[\"32\"]\n",
        "train_data_64, test_data_64 = data_dict[\"64\"]\n",
        "'''\n",
        "\n",
        "#MAX_LENGTH = max([len(x[0].split()) for x in train_data]) + 1\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "lang_dict = {}\n",
        "\n",
        "for split in splits:\n",
        "  lang_dict[\"train_in_\"+split] = Input(\"train_input_\"+split)\n",
        "  lang_dict[\"train_out_\"+split] = Output(\"train_output_\"+split)\n",
        "  lang_dict[\"test_in_\"+split] = Input(\"test_input_\"+split)\n",
        "  lang_dict[\"test_out_\"+split] = Output(\"test_output_\"+split)\n",
        "\n",
        "'''\n",
        "train_in = Input(\"train_input\")\n",
        "train_out = Output(\"train_output\")\n",
        "\n",
        "test_in = Input(\"test_input\")\n",
        "test_out = Output(\"test_output\")\n",
        "'''\n",
        "\n",
        "for split in splits:\n",
        "  train_data,test_data = data_dict[split]\n",
        "  for datapoint in train_data:\n",
        "          lang_dict[\"train_in_\"+split].addSentence(datapoint[0])\n",
        "          lang_dict[\"train_out_\"+split].addSentence(datapoint[1])\n",
        "\n",
        "  for datapoint in test_data:\n",
        "          lang_dict[\"test_in_\"+split].addSentence(datapoint[0])\n",
        "          lang_dict[\"test_out_\"+split].addSentence(datapoint[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L14fWhgqVcCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_test, test_data_test = data_dict[\"1\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyjch58TV3-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "badec97a-cca1-4f72-8360-af980f633316"
      },
      "source": [
        "train_data_test[0]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['jump opposite right twice and turn opposite right thrice',\n",
              "       'I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT'],\n",
              "      dtype='<U455')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66NZZ_dOKUg9",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "\n",
        "train_data = train_data[np.random.choice(train_data.shape[0], 100000, replace=True), :]\n",
        "\n",
        "def train_and_save(model, dropout, att, layers, model_name):\n",
        "    encoder = Encoder(train_in.n_words, 200, layers=layers, mode=model, dropout_p=dropout)\n",
        "    decoder = Decoder(200, train_out.n_words, layers=layers, max_length=MAX_LENGTH, mode=model, dropout_p=dropout, attention=att)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "    losses = trainIters(encoder, decoder, train_data, train_in, train_out, MAX_LENGTH, mode=model)\n",
        "    plt.plot(losses)\n",
        "    plt.title(model+'_layers='+str(layers)+'_drop='+str(dropout)+'_attention='+str(att))\n",
        "    plt.xlabel('iterations')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()\n",
        "    torch.save(encoder.state_dict(), \"/content/drive/My Drive/\"+model_name+\"_encoder.pt\")\n",
        "    torch.save(decoder.state_dict(), \"/content/drive/My Drive/\"+model_name+\"_decoder.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NwGTOeF7Jqp6",
        "outputId": "6e9e51c5-651b-46a1-852e-11dfffe163b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model='LSTM'\n",
        "for split in splits:\n",
        "  train_data, test_data = data_dict[split]\n",
        "  print(len(train_data))\n",
        "  print(len(test_data))\n",
        "  train_data = train_data[np.random.choice(train_data.shape[0], 100000, replace=True), :]\n",
        "  train_and_save(model, train_data, dropout=0.0, att=False, layers=2, model_name='LSTM_2nd_exercise_split_'+split)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n",
            "0\n",
            "0.004159884712912819\n",
            "500\n",
            "1.5738222190923563\n",
            "1000\n",
            "1.01875218774315\n",
            "1500\n",
            "0.6882623159278619\n",
            "2000\n",
            "0.5963558615379919\n",
            "2500\n",
            "0.530949799348667\n",
            "3000\n",
            "0.43277487046472873\n",
            "3500\n",
            "0.39760076227939656\n",
            "4000\n",
            "0.3083820516165601\n",
            "4500\n",
            "0.34996710250217317\n",
            "5000\n",
            "0.313941540263295\n",
            "5500\n",
            "0.2220824987940729\n",
            "6000\n",
            "0.21449277227146135\n",
            "6500\n",
            "0.21137796340782525\n",
            "7000\n",
            "0.19213516520676222\n",
            "7500\n",
            "0.1662076080163937\n",
            "8000\n",
            "0.14828307961569706\n",
            "8500\n",
            "0.16225096810966788\n",
            "9000\n",
            "0.12752832457243843\n",
            "9500\n",
            "0.08949467912290246\n",
            "10000\n",
            "0.12533550576513178\n",
            "10500\n",
            "0.09479370467376765\n",
            "11000\n",
            "0.10980401684176003\n",
            "11500\n",
            "0.08314503412068101\n",
            "12000\n",
            "0.06360003386814149\n",
            "12500\n",
            "0.07883179631970659\n",
            "13000\n",
            "0.08452450764626543\n",
            "13500\n",
            "0.0759088278149141\n",
            "14000\n",
            "0.06682923610935364\n",
            "14500\n",
            "0.0709308142944118\n",
            "15000\n",
            "0.08916552894269171\n",
            "15500\n",
            "0.09020297113811225\n",
            "16000\n",
            "0.04693329466332992\n",
            "16500\n",
            "0.0571551179733155\n",
            "17000\n",
            "0.08060473948413947\n",
            "17500\n",
            "0.03562725544693731\n",
            "18000\n",
            "0.026731629130538302\n",
            "18500\n",
            "0.04464385929265965\n",
            "19000\n",
            "0.04547590917126005\n",
            "19500\n",
            "0.04720857789361429\n",
            "20000\n",
            "0.07662165709506967\n",
            "20500\n",
            "0.024753620907223474\n",
            "21000\n",
            "0.048679111003227055\n",
            "21500\n",
            "0.07522943236083876\n",
            "22000\n",
            "0.0355470587879683\n",
            "22500\n",
            "0.07881133343918043\n",
            "23000\n",
            "0.04620911691884255\n",
            "23500\n",
            "0.02964643048636312\n",
            "24000\n",
            "0.06797994227340004\n",
            "24500\n",
            "0.06053892265815474\n",
            "25000\n",
            "0.036198456325339246\n",
            "25500\n",
            "0.02881413277828283\n",
            "26000\n",
            "0.03860473589000524\n",
            "26500\n",
            "0.024186115489850602\n",
            "27000\n",
            "0.03747220231389804\n",
            "27500\n",
            "0.05151431223696137\n",
            "28000\n",
            "0.024348881419510412\n",
            "28500\n",
            "0.05793014606779465\n",
            "29000\n",
            "0.024380702494424786\n",
            "29500\n",
            "0.02642658077134129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAGDCAYAAAD6aR7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDnGcejRkVncAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc_TWONzzvlW",
        "colab_type": "code",
        "outputId": "f3a4d1e8-0040-4cde-c048-0844d4b7c1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "encoder = Encoder(train_in.n_words, 200, layers=2, mode=\"LSTM\", dropout_p=0.0)\n",
        "encoder.load_state_dict(torch.load(\"/content/drive/My Drive/LSTM_no_drop_no_att_30k_encoder.pt\"))\n",
        "encoder.eval()\n",
        "\n",
        "decoder = Decoder(200, train_out.n_words, layers=2, max_length=MAX_LENGTH, mode=\"LSTM\", dropout_p=0.0, attention=False)\n",
        "decoder.load_state_dict(torch.load(\"/content/drive/My Drive/LSTM_no_drop_no_att_30k_decoder.pt\"))\n",
        "decoder.eval()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(8, 200)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (hidden_layer): LSTM(200, 200, num_layers=2)\n",
              "  (out): Linear(in_features=200, out_features=8, bias=True)\n",
              "  (softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pxmS4rEHL366",
        "outputId": "944abb23-f386-4863-b152-84aa981b3d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from data_loader import *\n",
        "#from embeddings import *\n",
        "#from layers_attempt import *\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device=torch.device(\"cpu\")\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length=100):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        #if torch.cuda.is_available():\n",
        "        #    input_tensor = input_tensor.cuda()\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden1 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "        encoder_hidden2 = torch.zeros(encoder.layers, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, (encoder_hidden1, encoder_hidden2) = encoder(input_tensor[ei],\n",
        "                                                     (encoder_hidden1, encoder_hidden2))\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "        decoder_hidden1 = encoder_hidden1\n",
        "        decoder_hidden2 = encoder_hidden2\n",
        "\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "          if decoder.attention:\n",
        "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs[:10]) # needs to only have the 10 in the encoder outputs?\n",
        "          else:\n",
        "            decoder_output, (decoder_hidden1, decoder_hidden2) = decoder(decoder_input, (decoder_hidden1, decoder_hidden2), encoder_outputs)\n",
        "          topv, topi = decoder_output.data.topk(1)\n",
        "          if topi.item() == EOS_token:\n",
        "              #decoded_words.append('<EOS>')\n",
        "              break\n",
        "          else:\n",
        "            decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "        return decoded_words\n",
        "\n",
        "def evaluateIters(test_data, encoder, decoder, lang_in, lang_out):\n",
        "    hit = 0\n",
        "    miss = 0\n",
        "    iters = 0\n",
        "    hit_idx = []\n",
        "    miss_idx = []\n",
        "\n",
        "    for idx, test_point in enumerate(test_data):\n",
        "        pred = evaluate(encoder, decoder, test_point[0], lang_in, lang_out)\n",
        "        pred = \" \".join(pred)\n",
        "        if pred == test_point[1]:\n",
        "            hit += 1\n",
        "            hit_idx.append(idx)\n",
        "        else:\n",
        "            miss += 1\n",
        "            miss_idx.append(idx)\n",
        "        iters += 1\n",
        "\n",
        "        if iters % 100 == 0:\n",
        "            print(iters)\n",
        "            print(hit)\n",
        "\n",
        "    return hit, hit_idx, miss, miss_idx\n",
        "\n",
        "hit, hit_idx, miss, miss_idx = evaluateIters(test_data, encoder, decoder, train_in, train_out)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "80\n",
            "200\n",
            "153\n",
            "300\n",
            "236\n",
            "400\n",
            "309\n",
            "500\n",
            "391\n",
            "600\n",
            "465\n",
            "700\n",
            "544\n",
            "800\n",
            "634\n",
            "900\n",
            "711\n",
            "1000\n",
            "790\n",
            "1100\n",
            "871\n",
            "1200\n",
            "951\n",
            "1300\n",
            "1029\n",
            "1400\n",
            "1105\n",
            "1500\n",
            "1184\n",
            "1600\n",
            "1270\n",
            "1700\n",
            "1350\n",
            "1800\n",
            "1426\n",
            "1900\n",
            "1499\n",
            "2000\n",
            "1572\n",
            "2100\n",
            "1657\n",
            "2200\n",
            "1727\n",
            "2300\n",
            "1807\n",
            "2400\n",
            "1890\n",
            "2500\n",
            "1965\n",
            "2600\n",
            "2047\n",
            "2700\n",
            "2132\n",
            "2800\n",
            "2211\n",
            "2900\n",
            "2288\n",
            "3000\n",
            "2367\n",
            "3100\n",
            "2441\n",
            "3200\n",
            "2513\n",
            "3300\n",
            "2588\n",
            "3400\n",
            "2672\n",
            "3500\n",
            "2755\n",
            "3600\n",
            "2839\n",
            "3700\n",
            "2909\n",
            "3800\n",
            "2986\n",
            "3900\n",
            "3068\n",
            "4000\n",
            "3143\n",
            "4100\n",
            "3217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "10WjnZrGR8aD",
        "outputId": "4e74cc53-f8a2-4181-c12c-15d1a335c555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "1-miss/test_data.shape[0]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7847919655667145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9auy0X5hXmD8",
        "outputId": "e95c910b-4b5c-42a0-f812-68c2522c1469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "idx=50\n",
        "print(test_data[miss_idx[idx]][0].split())\n",
        "print(evaluate(encoder, decoder, test_data[miss_idx[idx]][0], train_in, train_out))\n",
        "print(len(evaluate(encoder, decoder, test_data[miss_idx[idx]][0], train_in, train_out)))\n",
        "print(test_data[miss_idx[idx]][1].split())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['run', 'twice', 'after', 'jump', 'around', 'left']\n",
            "['I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_RUN', 'I_RUN', 'I_RUN']\n",
            "10\n",
            "['I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_TURN_LEFT', 'I_JUMP', 'I_RUN', 'I_RUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EG638DGIa-je",
        "outputId": "2ef8e446-d0c6-4e9e-df70-9386e416b213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data[4000][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'turn left thrice after turn opposite right twice'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFn0WQ1kU0VG",
        "outputId": "bd34bec0-3b01-4241-9f7a-1ceb74ab466c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max([len(x[0].split()) for x in train_data])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jJcgD6Z4VaFS",
        "outputId": "6e46323f-ccbf-4240-83b7-83c8762c3ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "str(True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'True'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSWSsqUHS3Rc",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}